{
 "cells": [
  {
   "cell_type": "code",
   "id": "da5f49cc-1ca4-47ed-9943-fae3287d32d0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-25T10:15:42.367631Z",
     "start_time": "2025-07-25T10:15:38.285462Z"
    }
   },
   "source": [
    "import os.path\n",
    "import tensorboardX\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "import torch.distributed as dist\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.utils.data import Dataset, DataLoader, DistributedSampler\n",
    "from model.HGNN import HGNN\n",
    "from utils.dataset import JobDataset\n",
    "from utils.metrics import *"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "3892844d-a1ca-4dd6-ad2e-9f468101ca30",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-25T10:15:46.683323Z",
     "start_time": "2025-07-25T10:15:46.649366Z"
    }
   },
   "source": [
    "def train(args):\n",
    "    # set random seed\n",
    "    torch.cuda.manual_seed_all(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "    np.random.seed(args.seed)\n",
    "\n",
    "    train_loader = DataLoader(JobDataset(root=\"E:/pycharm_projects/BISTRO/sparksteps/train/sample\",top_k=args.top_k, val_prop=args.val_prop), batch_size=args.batch_size, shuffle=True, num_workers=0)\n",
    "    test_loader = DataLoader(JobDataset(root=\"E:/pycharm_projects/BISTRO/sparksteps/test/sample\",top_k=args.top_k, mode='test', val_prop=args.val_prop), batch_size=args.batch_size, shuffle=True, num_workers=0)\n",
    "    in_channels = 0\n",
    "    out_channels = args.n_hid\n",
    "    ncount = 0\n",
    "    for i, data in enumerate(train_loader):\n",
    "        phi1 = data['phi1']\n",
    "        phi1_inv = data['phi1_inverse']\n",
    "        phi2 = data['phi2']\n",
    "        phi2_inv = data['phi2_inverse']\n",
    "        fea = data['Fea']\n",
    "        joblst = data['joblst']\n",
    "        label = data['label']\n",
    "        in_channels = fea.shape[-1]\n",
    "        ncount = phi1.shape[-1]\n",
    "        break\n",
    "\n",
    "\n",
    "    model = HGNN(in_channels, out_channels, ncount, args.device, args.top_k)\n",
    "    if args.device != 'cpu':\n",
    "        model = model.to(args.device)\n",
    "\n",
    "    loss_func = nn.CrossEntropyLoss()\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(),\n",
    "                                      lr=args.lr,\n",
    "                                      weight_decay=args.weight_decay)\n",
    "\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=args.stepsize, gamma=args.gamma)\n",
    "\n",
    "    model_path = './save_models/'\n",
    "    log_path = './logs'\n",
    "    if not os.path.exists(model_path):\n",
    "        os.makedirs(model_path)\n",
    "    elif os.path.exists(model_path + 'parameter.pkl'):\n",
    "        model.load_state_dict(torch.load(model_path + 'parameter.pkl'))\n",
    "    if not os.path.exists(log_path):\n",
    "        os.makedirs(log_path)\n",
    "    writer = tensorboardX.SummaryWriter(log_path)\n",
    "\n",
    "    step_n = 0\n",
    "    best_hr = 0.0\n",
    "    best_mrr = 0.0\n",
    "    for epoch in range(args.n_epoch):\n",
    "#         print(\"epoch is:\", epoch)\n",
    "        model.train()\n",
    "\n",
    "        progress_bar = tqdm(enumerate(train_loader), total=len(train_loader), desc=f\"Epoch {epoch + 1}\")\n",
    "        for batch_idx, data in progress_bar: # tqdm(train_loader, desc=f\"Epoch {epoch+1}\"):\n",
    "            phi1 = data['phi1']\n",
    "            phi1_inv = data['phi1_inverse']\n",
    "            phi2 = data['phi2']\n",
    "            phi2_inv = data['phi2_inverse']\n",
    "            fea = data['Fea']\n",
    "            joblst = data['joblst']\n",
    "            label = data['label']\n",
    "            # label = label.unsqueeze(1)\n",
    "\n",
    "            if args.device != 'cpu':\n",
    "                phi1 = phi1.to(args.device)\n",
    "                phi1_inv = phi1_inv.to(args.device)\n",
    "                phi2 = phi2.to(args.device)\n",
    "                phi2_inv = phi2_inv.to(args.device)\n",
    "                fea = fea.to(args.device)\n",
    "                joblst = joblst.to(args.device)\n",
    "                label = label.to(args.device)\n",
    "\n",
    "            output = model.forward(phi1, phi1_inv, phi2, phi2_inv, fea, joblst)\n",
    "            # print('main output shape:', output.shape)\n",
    "            # print('main label shape:', label.shape)\n",
    "            # print(label)\n",
    "            loss = loss_func(output, label)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            _, pred = torch.max(output.data, dim=1)\n",
    "            correct = pred.eq(label.data).cpu().sum()\n",
    "\n",
    "            writer.add_scalar(\"train loss\", loss.item(), global_step=step_n)\n",
    "            writer.add_scalar(\"train correct\", 100.0 * correct.item() / args.batch_size, global_step=step_n)\n",
    "            progress_bar.set_description(f\"Epoch {epoch + 1} loss={round(loss.item(), 8)}\")\n",
    "            step_n += 1\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        sum_loss = 0\n",
    "        model.eval()\n",
    "        hit_rates = []\n",
    "        average_precisions = []\n",
    "        precisions = []\n",
    "        recalls = []\n",
    "        f1_scores = []\n",
    "        mrrs = []\n",
    "        ndcgs = []\n",
    "        for i, data in enumerate(test_loader):\n",
    "            phi1 = data['phi1']\n",
    "            phi1_inv = data['phi1_inverse']\n",
    "            phi2 = data['phi2']\n",
    "            phi2_inv = data['phi2_inverse']\n",
    "            fea = data['Fea']\n",
    "            joblst = data['joblst']\n",
    "            label = data['label']\n",
    "\n",
    "            if args.device != 'cpu':\n",
    "                phi1 = phi1.to(args.device)\n",
    "                phi1_inv = phi1_inv.to(args.device)\n",
    "                phi2 = phi2.to(args.device)\n",
    "                phi2_inv = phi2_inv.to(args.device)\n",
    "                fea = fea.to(args.device)\n",
    "                joblst = joblst.to(args.device)\n",
    "                label = label.to(args.device)\n",
    "\n",
    "            output = model(phi1, phi1_inv, phi2, phi2_inv, fea, joblst)\n",
    "            loss = loss_func(output, label)\n",
    "            _, pred = torch.max(output.data, dim=1)\n",
    "\n",
    "            for i in range(pred.shape[0]):\n",
    "                recommended_items = pred.tolist()[i]\n",
    "                test_items = label.tolist()[i]\n",
    "                hit_rates.append(hit_rate(recommended_items, test_items))\n",
    "                average_precisions.append(average_precision(recommended_items, test_items))\n",
    "                pre = precision(recommended_items, test_items)\n",
    "                rec = recall(recommended_items, test_items)\n",
    "                precisions.append(pre)\n",
    "                recalls.append(rec)\n",
    "                f1_scores.append(f1_score(pre, rec))\n",
    "                mrrs.append(mean_reciprocal_rank(recommended_items, test_items))\n",
    "                ndcgs.append(ndcg(recommended_items, test_items))\n",
    "\n",
    "            sum_loss += loss.item()\n",
    "        test_loss = sum_loss * 1.0 / len(test_loader)\n",
    "\n",
    "        # 计算总体指标\n",
    "        overall_mrr = np.mean(mrrs)\n",
    "        overall_hr = np.mean(hit_rates)\n",
    "\n",
    "        writer.add_scalar(\"test loss\", test_loss, global_step=epoch + 1)\n",
    "        writer.add_scalar(\"test Mean Reciprocal Rank\", overall_mrr, global_step=epoch + 1)\n",
    "        writer.add_scalar(\"test Hit Rate\", overall_hr, global_step=epoch + 1)\n",
    "        \n",
    "        flag = False\n",
    "        if best_hr < overall_hr:\n",
    "            best_hr = overall_hr\n",
    "            flag = True\n",
    "        if best_mrr < overall_mrr:\n",
    "            best_mrr = overall_mrr\n",
    "            flag = True\n",
    "        \n",
    "        if flag:\n",
    "            print(\"Best Hit Rate is\", best_hr, 'Best Mean Reciprocal Rank:', best_mrr)\n",
    "    return"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "d2cd0fed-1d0a-4ed7-b676-1c394ede1047",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-25T10:15:58.751287Z",
     "start_time": "2025-07-25T10:15:56.534725Z"
    }
   },
   "source": [
    "import argparse\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser(description='test')\n",
    "    parser.add_argument('--model', type=str, default='MKGNN')\n",
    "    parser.add_argument('--clip_num', type=float, default=0.0)\n",
    "    parser.add_argument('--cuda', type=int, default=2)\n",
    "    parser.add_argument('--order', type=int, default=3)\n",
    "    parser.add_argument('--dp', type=float, default=0.8)\n",
    "    parser.add_argument('--n_hid', type=int, default=64)\n",
    "    parser.add_argument('--use_bias', type=bool, default=True)\n",
    "    parser.add_argument('--top_k', type=int, default=10)\n",
    "    parser.add_argument('--val_prop', type=float, default=0.1)\n",
    "    parser.add_argument('--batch_size', type=int, default=16)\n",
    "    parser.add_argument('--k_job', type=int, default=500)\n",
    "    parser.add_argument('--k_person', type=int, default=1000)\n",
    "    parser.add_argument('--seed', type=int, default=101)\n",
    "    parser.add_argument('--lr', type=float, default=0.001)\n",
    "    parser.add_argument('--n_epoch', type=int, default=10000)\n",
    "    parser.add_argument('--weight_decay', type=float, default=0.0001)\n",
    "    parser.add_argument('--gamma', type=float, default=0.99)\n",
    "    parser.add_argument('--stepsize', type=int, default=1000)\n",
    "    parser.add_argument('--beta_s', type=float, default=0.4)\n",
    "    parser.add_argument('--beta_e', type=float, default=0.9999)\n",
    "    args = parser.parse_args(args=[])\n",
    "    # print('args:', args)\n",
    "    \n",
    "    args.device = torch.device(\"cpu\")\n",
    "    if args.cuda >= 0:\n",
    "        args.device = torch.device(\"cuda:\" + str(args.cuda))\n",
    "    train(args)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train sample count: 528\n",
      "test sample count: 35\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Batches of sparse tensors are not currently supported by the default collate_fn; please provide a custom collate_fn to handle them appropriately.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[3], line 31\u001B[0m\n\u001B[0;32m     29\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m args\u001B[38;5;241m.\u001B[39mcuda \u001B[38;5;241m>\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[0;32m     30\u001B[0m     args\u001B[38;5;241m.\u001B[39mdevice \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mdevice(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcuda:\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mstr\u001B[39m(args\u001B[38;5;241m.\u001B[39mcuda))\n\u001B[1;32m---> 31\u001B[0m \u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43margs\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[2], line 12\u001B[0m, in \u001B[0;36mtrain\u001B[1;34m(args)\u001B[0m\n\u001B[0;32m     10\u001B[0m out_channels \u001B[38;5;241m=\u001B[39m args\u001B[38;5;241m.\u001B[39mn_hid\n\u001B[0;32m     11\u001B[0m ncount \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[1;32m---> 12\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i, data \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(train_loader):\n\u001B[0;32m     13\u001B[0m     phi1 \u001B[38;5;241m=\u001B[39m data[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mphi1\u001B[39m\u001B[38;5;124m'\u001B[39m]\n\u001B[0;32m     14\u001B[0m     phi1_inv \u001B[38;5;241m=\u001B[39m data[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mphi1_inverse\u001B[39m\u001B[38;5;124m'\u001B[39m]\n",
      "File \u001B[1;32mE:\\pycharm_projects\\BISTRO\\.venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:733\u001B[0m, in \u001B[0;36m_BaseDataLoaderIter.__next__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    730\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sampler_iter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    731\u001B[0m     \u001B[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001B[39;00m\n\u001B[0;32m    732\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reset()  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n\u001B[1;32m--> 733\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_next_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    734\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m    735\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[0;32m    736\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_kind \u001B[38;5;241m==\u001B[39m _DatasetKind\u001B[38;5;241m.\u001B[39mIterable\n\u001B[0;32m    737\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    738\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m>\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called\n\u001B[0;32m    739\u001B[0m ):\n",
      "File \u001B[1;32mE:\\pycharm_projects\\BISTRO\\.venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:789\u001B[0m, in \u001B[0;36m_SingleProcessDataLoaderIter._next_data\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    787\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21m_next_data\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    788\u001B[0m     index \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_next_index()  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[1;32m--> 789\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_dataset_fetcher\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfetch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mindex\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[0;32m    790\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory:\n\u001B[0;32m    791\u001B[0m         data \u001B[38;5;241m=\u001B[39m _utils\u001B[38;5;241m.\u001B[39mpin_memory\u001B[38;5;241m.\u001B[39mpin_memory(data, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory_device)\n",
      "File \u001B[1;32mE:\\pycharm_projects\\BISTRO\\.venv\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:55\u001B[0m, in \u001B[0;36m_MapDatasetFetcher.fetch\u001B[1;34m(self, possibly_batched_index)\u001B[0m\n\u001B[0;32m     53\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m     54\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[possibly_batched_index]\n\u001B[1;32m---> 55\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcollate_fn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mE:\\pycharm_projects\\BISTRO\\.venv\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:398\u001B[0m, in \u001B[0;36mdefault_collate\u001B[1;34m(batch)\u001B[0m\n\u001B[0;32m    337\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mdefault_collate\u001B[39m(batch):\n\u001B[0;32m    338\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    339\u001B[0m \u001B[38;5;124;03m    Take in a batch of data and put the elements within the batch into a tensor with an additional outer dimension - batch size.\u001B[39;00m\n\u001B[0;32m    340\u001B[0m \n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    396\u001B[0m \u001B[38;5;124;03m        >>> default_collate(batch)  # Handle `CustomType` automatically\u001B[39;00m\n\u001B[0;32m    397\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 398\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mcollate\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcollate_fn_map\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdefault_collate_fn_map\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mE:\\pycharm_projects\\BISTRO\\.venv\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:171\u001B[0m, in \u001B[0;36mcollate\u001B[1;34m(batch, collate_fn_map)\u001B[0m\n\u001B[0;32m    165\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(elem, collections\u001B[38;5;241m.\u001B[39mabc\u001B[38;5;241m.\u001B[39mMutableMapping):\n\u001B[0;32m    166\u001B[0m     \u001B[38;5;66;03m# The mapping type may have extra properties, so we can't just\u001B[39;00m\n\u001B[0;32m    167\u001B[0m     \u001B[38;5;66;03m# use `type(data)(...)` to create the new mapping.\u001B[39;00m\n\u001B[0;32m    168\u001B[0m     \u001B[38;5;66;03m# Create a clone and update it if the mapping type is mutable.\u001B[39;00m\n\u001B[0;32m    169\u001B[0m     clone \u001B[38;5;241m=\u001B[39m copy\u001B[38;5;241m.\u001B[39mcopy(elem)\n\u001B[0;32m    170\u001B[0m     clone\u001B[38;5;241m.\u001B[39mupdate(\n\u001B[1;32m--> 171\u001B[0m         {\n\u001B[0;32m    172\u001B[0m             key: collate(\n\u001B[0;32m    173\u001B[0m                 [d[key] \u001B[38;5;28;01mfor\u001B[39;00m d \u001B[38;5;129;01min\u001B[39;00m batch], collate_fn_map\u001B[38;5;241m=\u001B[39mcollate_fn_map\n\u001B[0;32m    174\u001B[0m             )\n\u001B[0;32m    175\u001B[0m             \u001B[38;5;28;01mfor\u001B[39;00m key \u001B[38;5;129;01min\u001B[39;00m elem\n\u001B[0;32m    176\u001B[0m         }\n\u001B[0;32m    177\u001B[0m     )\n\u001B[0;32m    178\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m clone\n\u001B[0;32m    179\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "File \u001B[1;32mE:\\pycharm_projects\\BISTRO\\.venv\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:172\u001B[0m, in \u001B[0;36m<dictcomp>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m    165\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(elem, collections\u001B[38;5;241m.\u001B[39mabc\u001B[38;5;241m.\u001B[39mMutableMapping):\n\u001B[0;32m    166\u001B[0m     \u001B[38;5;66;03m# The mapping type may have extra properties, so we can't just\u001B[39;00m\n\u001B[0;32m    167\u001B[0m     \u001B[38;5;66;03m# use `type(data)(...)` to create the new mapping.\u001B[39;00m\n\u001B[0;32m    168\u001B[0m     \u001B[38;5;66;03m# Create a clone and update it if the mapping type is mutable.\u001B[39;00m\n\u001B[0;32m    169\u001B[0m     clone \u001B[38;5;241m=\u001B[39m copy\u001B[38;5;241m.\u001B[39mcopy(elem)\n\u001B[0;32m    170\u001B[0m     clone\u001B[38;5;241m.\u001B[39mupdate(\n\u001B[0;32m    171\u001B[0m         {\n\u001B[1;32m--> 172\u001B[0m             key: \u001B[43mcollate\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    173\u001B[0m \u001B[43m                \u001B[49m\u001B[43m[\u001B[49m\u001B[43md\u001B[49m\u001B[43m[\u001B[49m\u001B[43mkey\u001B[49m\u001B[43m]\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43md\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcollate_fn_map\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcollate_fn_map\u001B[49m\n\u001B[0;32m    174\u001B[0m \u001B[43m            \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    175\u001B[0m             \u001B[38;5;28;01mfor\u001B[39;00m key \u001B[38;5;129;01min\u001B[39;00m elem\n\u001B[0;32m    176\u001B[0m         }\n\u001B[0;32m    177\u001B[0m     )\n\u001B[0;32m    178\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m clone\n\u001B[0;32m    179\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "File \u001B[1;32mE:\\pycharm_projects\\BISTRO\\.venv\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:155\u001B[0m, in \u001B[0;36mcollate\u001B[1;34m(batch, collate_fn_map)\u001B[0m\n\u001B[0;32m    153\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m collate_fn_map \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    154\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m elem_type \u001B[38;5;129;01min\u001B[39;00m collate_fn_map:\n\u001B[1;32m--> 155\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mcollate_fn_map\u001B[49m\u001B[43m[\u001B[49m\u001B[43melem_type\u001B[49m\u001B[43m]\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcollate_fn_map\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcollate_fn_map\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    157\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m collate_type \u001B[38;5;129;01min\u001B[39;00m collate_fn_map:\n\u001B[0;32m    158\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(elem, collate_type):\n",
      "File \u001B[1;32mE:\\pycharm_projects\\BISTRO\\.venv\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:262\u001B[0m, in \u001B[0;36mcollate_tensor_fn\u001B[1;34m(batch, collate_fn_map)\u001B[0m\n\u001B[0;32m    251\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[0;32m    252\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mBatches of nested tensors are not currently supported by the default collate_fn; \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    253\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mplease provide a custom collate_fn to handle them appropriately.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    254\u001B[0m     )\n\u001B[0;32m    255\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m elem\u001B[38;5;241m.\u001B[39mlayout \u001B[38;5;129;01min\u001B[39;00m {\n\u001B[0;32m    256\u001B[0m     torch\u001B[38;5;241m.\u001B[39msparse_coo,\n\u001B[0;32m    257\u001B[0m     torch\u001B[38;5;241m.\u001B[39msparse_csr,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    260\u001B[0m     torch\u001B[38;5;241m.\u001B[39msparse_bsc,\n\u001B[0;32m    261\u001B[0m }:\n\u001B[1;32m--> 262\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[0;32m    263\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mBatches of sparse tensors are not currently supported by the default collate_fn; \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    264\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mplease provide a custom collate_fn to handle them appropriately.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    265\u001B[0m     )\n\u001B[0;32m    266\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mutils\u001B[38;5;241m.\u001B[39mdata\u001B[38;5;241m.\u001B[39mget_worker_info() \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    267\u001B[0m     \u001B[38;5;66;03m# If we're in a background process, concatenate directly into a\u001B[39;00m\n\u001B[0;32m    268\u001B[0m     \u001B[38;5;66;03m# shared memory tensor to avoid an extra copy\u001B[39;00m\n\u001B[0;32m    269\u001B[0m     numel \u001B[38;5;241m=\u001B[39m \u001B[38;5;28msum\u001B[39m(x\u001B[38;5;241m.\u001B[39mnumel() \u001B[38;5;28;01mfor\u001B[39;00m x \u001B[38;5;129;01min\u001B[39;00m batch)\n",
      "\u001B[1;31mRuntimeError\u001B[0m: Batches of sparse tensors are not currently supported by the default collate_fn; please provide a custom collate_fn to handle them appropriately."
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e68fa6-58a1-44e4-8142-2dc89355a0ed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
